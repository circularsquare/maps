{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de2dc221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19124/342908967.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mosmnx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\osmnx\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbearing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbearing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistance\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0melevation\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0melevation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\osmnx\\distance.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# scikit-learn is optional dependency for unprojected nearest-neighbor search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mBallTree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m from .utils._tags import (\n\u001b[0;32m     19\u001b[0m     \u001b[0m_DEFAULT_TAGS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\utils\\murmurhash.pyx\u001b[0m in \u001b[0;36minit sklearn.utils.murmurhash\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm, NoNorm, PowerNorm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.spatial import KDTree\n",
    "from itertools import combinations\n",
    "from pyproj import Transformer\n",
    "from shapely import LineString, MultiLineString\n",
    "from shapely.geometry import Point, box\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411ba602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\anita\\appdata\\roaming\\python\\python39\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\anita\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anita\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anita\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anita\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anita\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15152d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f7ac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\anita\\\\anaconda3', 'C:\\\\Users\\\\anita\\\\anaconda3\\\\lib\\\\site-packages']\n"
     ]
    }
   ],
   "source": [
    "import site\n",
    "print(site.getsitepackages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a13c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0c73f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da0f19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anita\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 2.0.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19568/3753013009.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mosmnx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\osmnx\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbearing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbearing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistance\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0melevation\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0melevation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\osmnx\\distance.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# scipy is optional dependency for projected nearest-neighbor search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcKDTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mcKDTree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# noqa: N816\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \"\"\"\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mkdtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mckdtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mqhull\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\kdtree.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mckdtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcKDTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcKDTreeNode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m __all__ = ['minkowski_distance_p', 'minkowski_distance',\n",
      "\u001b[1;32mckdtree.pyx\u001b[0m in \u001b[0;36minit scipy.spatial.ckdtree\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "SUBWAY_SPEED = 3 # used in euclidean dist\n",
    "TARGET_CRS = \"EPSG:3095\"\n",
    "\n",
    "def jittered_weight(u, v, data):\n",
    "    jitterSize = 0.03\n",
    "    base_length = data['length']\n",
    "    jitter = random.gauss(1, jitterSize)\n",
    "    return base_length * jitter\n",
    "def jitter_sqrt(u, v, data):\n",
    "    strength = 0.5 # strength of 1 means 10 m path will be jittered by 1 * sqrt(10) = 3\n",
    "                # or 100 m path will get jittered by 1 * sqrt (100) = 10\n",
    "    base_length = data['length']\n",
    "    if base_length <= 0:\n",
    "        return 0.001\n",
    "    sigma = strength * math.sqrt(base_length)\n",
    "    noise = random.gauss(0, sigma)\n",
    "    return max(0.001, base_length + noise)\n",
    "def euclidean_dist(u, v):\n",
    "    return np.linalg.norm(np.array(u) - np.array(v)) / SUBWAY_SPEED \n",
    "\n",
    "def pathfind(source_target_pair, graph, weight_func, heuristic_func, exponent_val):\n",
    "    source_node, target_node = source_target_pair\n",
    "    source_node = tuple(source_node)\n",
    "    target_node = tuple(target_node)\n",
    "    try: \n",
    "        path = nx.astar_path(graph, source_node, target_node, \n",
    "                            weight=weight_func, heuristic=heuristic_func)\n",
    "        network_dist = sum(graph.edges[u, v]['length'] for u, v in zip(path[:-1], path[1:]))\n",
    "        distance = np.linalg.norm(np.array(source_node) - np.array(target_node))\n",
    "        directness = distance / (network_dist + 0.001)\n",
    "        influence = directness ** exponent_val\n",
    "        path_edges = list(zip(path[:-1], path[1:]))\n",
    "        # Return all the edges from this path and the influence score for this path\n",
    "        return (path_edges, influence)\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "def offset_geometries(gdf, offset_distance):\n",
    "    new_geometries = []\n",
    "    # Center the offsets around zero\n",
    "    start_offset = -offset_distance * (len(gdf) - 1) / 2\n",
    "    for i, row in enumerate(gdf.itertuples()):\n",
    "        # Calculate the offset for this specific line\n",
    "        current_offset = start_offset + i * offset_distance\n",
    "        new_geom = row.geometry.parallel_offset(current_offset, 'left', join_style=2)\n",
    "        new_geometries.append(new_geom)\n",
    "    return new_geometries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe0cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95fccd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findUsage(num_iterations = 10000000):\n",
    "    print(\"Downloading and building graph from OpenStreetMap...\")\n",
    "    place_name = \"Tokyo, Japan\"\n",
    "    bbox = (139.546967,35.524403,139.943848,35.842308)\n",
    "    center_lon, center_lat = (bbox[0]+bbox[2])/2, (bbox[1]+bbox[3])/2\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", TARGET_CRS, always_xy = True)\n",
    "    center_x, center_y = transformer.transform(center_lon, center_lat)\n",
    "    center_point = np.array([center_x, center_y])\n",
    "\n",
    "    # --- 1. Get data by uncommenting below 4 lines! ---\n",
    "\n",
    "    # G_directed = ox.graph_from_bbox(bbox = bbox, network_type='drive')\n",
    "    # G_proj = ox.project_graph(G_directed, to_crs=TARGET_CRS)\n",
    "    # with open('graph_tokyo.pkl', 'wb') as f:\n",
    "    #     pickle.dump(G_proj, f)\n",
    "\n",
    "    with open('graph_tokyo.pkl', 'rb') as f:\n",
    "        G_proj = pickle.load(f)\n",
    "\n",
    "    G_initial = nx.Graph(G_proj)\n",
    "    print(f\"Downloaded graph with {nx.number_connected_components(G_initial)} components.\")\n",
    "\n",
    "    # --- 2. Convert Graph Nodes to Coordinate Tuples ---\n",
    "    print(\"Converting graph nodes from OSM IDs to coordinates...\")\n",
    "    pos = {node: (data['x'], data['y']) for node, data in G_initial.nodes(data=True)}\n",
    "    G = nx.Graph() # This will be our final graph with coordinate nodes\n",
    "    for u, v, data in G_initial.edges(data=True):\n",
    "        u_coords = pos[u]\n",
    "        v_coords = pos[v]\n",
    "        G.add_edge(u_coords, v_coords, **data)\n",
    "    street_nodes = list(G.nodes())\n",
    "\n",
    "    # # --- 2.5. subway! ---\n",
    "    print('loading subway data...')\n",
    "\n",
    "    station_tags = {'railway': 'station', 'station': 'subway'}\n",
    "    stations_gdf = ox.features_from_bbox(bbox, station_tags)\n",
    "    stations_gdf = stations_gdf.to_crs(TARGET_CRS)\n",
    "    # Filter for valid Point geometries\n",
    "    stations_gdf = stations_gdf[stations_gdf.geometry.type == 'Point']\n",
    "    print(f\"Downloaded {len(stations_gdf)} subway stations.\")\n",
    "\n",
    "    line_tags = {'railway': 'subway'}\n",
    "    lines_gdf = ox.features_from_bbox(bbox, line_tags)\n",
    "    lines_gdf = lines_gdf.to_crs(TARGET_CRS)\n",
    "    print(f\"Downloaded {len(lines_gdf)} subway line segments.\")\n",
    "\n",
    "    print(lines_gdf.columns)\n",
    "    print(stations_gdf.columns)\n",
    "    print(stations_gdf.iloc[0])\n",
    "    print(lines_gdf.iloc[0])\n",
    "\n",
    "\n",
    "    # prepare street graph G\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        G.edges[u, v]['type'] = 'street'\n",
    "\n",
    "    subG = nx.Graph()\n",
    "\n",
    "    all_subway_data = [] # We'll create a list of dictionaries\n",
    "    for index, row in lines_gdf.iterrows():\n",
    "        line = row.geometry\n",
    "        route_id = row['route_id'] \n",
    "        segments = [LineString(pair) for pair in zip(line.coords, line.coords[1:])]\n",
    "        for seg in segments:\n",
    "            all_subway_data.append({'geometry': seg, 'route_id': route_id})\n",
    "    segmented_gdf = gpd.GeoDataFrame(all_subway_data, crs=TARGET_CRS)\n",
    "\n",
    "    # --- Part 2: Build the graph from our new, detailed list ---\n",
    "    for seg_data in all_subway_data:\n",
    "        segment = seg_data['geometry']\n",
    "        route_id = seg_data['route_id'] \n",
    "        start_node = segment.coords[0][:2]\n",
    "        end_node = segment.coords[-1][:2]\n",
    "        if start_node != end_node:\n",
    "            travel_time = segment.length / (SUBWAY_SPEED * subway_speeds[route_id])\n",
    "            subG.add_edge(start_node, end_node,\n",
    "                        length=travel_time, \n",
    "                        type='subway',\n",
    "                        route_id=route_id) \n",
    "\n",
    "    # 3. find and bridge gaps between segment!\n",
    "    endpoints = []\n",
    "    for index, row in lines_gdf.iterrows():\n",
    "        line = row.geometry\n",
    "        if len(line.coords) > 1: # Ensure the line is valid\n",
    "            endpoints.append(line.coords[0])\n",
    "            endpoints.append(line.coords[-1])\n",
    "    unique_endpoints = list(set(endpoints))\n",
    "    endpoint_tree = KDTree(unique_endpoints)\n",
    "\n",
    "    # Find all pairs of endpoints that are within 20 meters of each other\n",
    "    gap_pairs = endpoint_tree.query_pairs(r=20)\n",
    "    for (i, j) in gap_pairs:\n",
    "        node1 = unique_endpoints[i]\n",
    "        node2 = unique_endpoints[j]\n",
    "        dist = euclidean_dist(node1, node2)\n",
    "        subG.add_edge(node1, node2, length = dist / SUBWAY_SPEED, type='subway')\n",
    "\n",
    "    G = nx.compose(G, subG)\n",
    "\n",
    "    # connecting stations!\n",
    "    ENTER_STATION = 30 # meters from station to street (walking is ~ 1 m/s)\n",
    "    TRAIN_WAIT = 400 # meters from station to getting on train (applied both ways)\n",
    "\n",
    "    print(\"Connecting networks at station entrances...\")\n",
    "    station_id_to_node = {}\n",
    "    for station in stations_gdf.itertuples():\n",
    "        platform_coords = (station.geometry.x, station.geometry.y)\n",
    "        G.add_node(platform_coords)\n",
    "        station_id_to_node[station.station_id] = platform_coords\n",
    "\n",
    "    subway_nodes = [n for n, d in G.nodes(data=True) if G.degree(n) > 0 and all(G.edges[n, neighbor].get('type') == 'subway' for neighbor in G.neighbors(n))]\n",
    "    subway_tree = KDTree(subway_nodes)\n",
    "\n",
    "    street_tree = KDTree(street_nodes) # connect station to street\n",
    "    for station_id, platform_node in station_id_to_node.items():\n",
    "        dist, idx = street_tree.query(platform_node)\n",
    "        nearest_street_node = street_nodes[idx]\n",
    "        entrance_time = dist + ENTER_STATION\n",
    "        G.add_edge(platform_node, nearest_street_node, length = dist + ENTER_STATION, type='station_entrance')\n",
    "\n",
    "    for station in stations_gdf.itertuples():\n",
    "        for route in station.routes:\n",
    "            try:\n",
    "                # 1. Find ALL possible line segments for the current route\n",
    "                platform_coords = (station.geometry.x, station.geometry.y)\n",
    "                platform_point = Point(platform_coords)\n",
    "\n",
    "                possible_lines = lines_gdf[lines_gdf['route_id'] == route]\n",
    "                if possible_lines.empty:\n",
    "                    print('route not found ' + route)\n",
    "                route_geom = possible_lines.geometry.union_all()\n",
    "                track_point = route_geom.interpolate(route_geom.project(platform_point))\n",
    "                fallback = False\n",
    "                if track_point:\n",
    "                    track_node_coords = (track_point.x, track_point.y)\n",
    "                    dist, idx = subway_tree.query(track_node_coords)\n",
    "                    nearest_track_node_on_route = subway_nodes[idx]\n",
    "                    platform_track_dist = euclidean_dist(platform_coords, nearest_track_node_on_route)\n",
    "                else: \n",
    "                    fallback = True \n",
    "                if fallback or platform_track_dist > 200 and route != '5':\n",
    "                    print(f\"warning: Large platform-to-track distance of {platform_track_dist:.0f}m \"\n",
    "                        f\"for route {route} at {station.stop_name}\")\n",
    "                    # fallback if interpolate doesn't work: find closest node\n",
    "                    route_nodes = []\n",
    "                    if isinstance(route_geom, MultiLineString):\n",
    "                        for line in route_geom.geoms:\n",
    "                            route_nodes.extend(line.coords)\n",
    "                    else:\n",
    "                        route_nodes.extend(route_geom.coords)\n",
    "                    route_tree = KDTree(route_nodes)\n",
    "                    dist, idx = route_tree.query(platform_coords)\n",
    "                    nearest_track_node_on_route = route_nodes[idx]\n",
    "                # Add an edge from the platform to the specific track\n",
    "                G.add_edge(platform_coords, nearest_track_node_on_route, \n",
    "                        length=TRAIN_WAIT + platform_track_dist*1.5, type='platform_access')\n",
    "                #print('added edge ' + route + station.stop_name)\n",
    "            except (IndexError, KeyError): # if route doesn't have a geometry\n",
    "                print(\"route doesn't have geometry \" + route)\n",
    "                pass\n",
    "\n",
    "    # adding stop time penalties\n",
    "    STOP_TIME = 20 # applied twice while passing a station\n",
    "    track_nodes_with_stops = set()\n",
    "    for station_id, platform_node in station_id_to_node.items():\n",
    "        for neighbor in G.neighbors(platform_node):\n",
    "            if G.edges[platform_node, neighbor].get('type') == 'platform_access':\n",
    "                track_nodes_with_stops.add(neighbor)\n",
    "    for track_node in track_nodes_with_stops:\n",
    "        for neighbor in G.neighbors(track_node):\n",
    "            edge_data = G.edges[track_node, neighbor]\n",
    "            if edge_data.get('type') == 'subway':\n",
    "                edge_data['length'] += STOP_TIME\n",
    "        \n",
    "    print(\"Multi-modal graph creation complete!\")\n",
    "\n",
    "    # --- 2.6 load population! --\n",
    "    print('loading population...')\n",
    "    kontur_filepath = \"data/nystreets/kontur_population_JP_20231101.gpkg\"\n",
    "    west, south, east, north = bbox #139.546967,35.524403,139.943848,35.842308\n",
    "    bbox_gdf_latlon = gpd.GeoDataFrame(\n",
    "        geometry=[box(west, south, east, north)],\n",
    "        crs=\"EPSG:4326\")\n",
    "    bbox_gdf_projected = bbox_gdf_latlon.to_crs(TARGET_CRS)\n",
    "    projected_bbox_coords = tuple(bbox_gdf_projected.total_bounds)\n",
    "    pop_gdf = gpd.read_file(\n",
    "        kontur_filepath,\n",
    "        bbox=projected_bbox_coords).to_crs(TARGET_CRS)\n",
    "    pop_centers_gdf = pop_gdf.copy() # collapse to centers\n",
    "    pop_centers_gdf['geometry'] = pop_centers_gdf.geometry.centroid\n",
    "    pop_points = np.array([p.coords[0] for p in pop_centers_gdf.geometry])\n",
    "    pop_values = pop_centers_gdf['population'].values\n",
    "    pop_tree = KDTree(pop_points)\n",
    "    print(f\"Data loaded successfully. Found {len(pop_gdf)} population grid cells.\")\n",
    "\n",
    "    # --- 3. Run Pathfinding Simulation ---\n",
    "    print(\"Selecting paths...\")\n",
    "    all_nodes = list(G.nodes)\n",
    "    all_edges = list(G.edges)\n",
    "    final_weights = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        G.edges[u, v]['usage'] = 0\n",
    "        # G.edges[u, v]['length'] = G.edges[u, v]['length']\n",
    "\n",
    "\n",
    "    shortDistance = 1000\n",
    "    exponent = 2.3  \n",
    "    proximity_sigma = 27000 # 1.6 km per mile\n",
    "\n",
    "    # calculate probability of being picked of edges\n",
    "    u_coords = np.array([u for u, v in all_edges])\n",
    "    v_coords = np.array([v for u, v in all_edges])\n",
    "    midpoints = (u_coords + v_coords)/2\n",
    "    lengths = np.array(list(nx.get_edge_attributes(G, 'length').values()))\n",
    "    # dist_center = np.linalg.norm(u_coords - center_point, axis=1)\n",
    "    prox_weight = 1 #np.exp(-(dist_center**2) / (2 * proximity_sigma**2))\n",
    "\n",
    "    pop_distances, pop_indices = pop_tree.query(midpoints, k=1)\n",
    "    edge_pops = pop_values[pop_indices]\n",
    "    # normalize by total edge length in hexagon\n",
    "    temp_df = pd.DataFrame({'edge_length': lengths, 'population': edge_pops, 'hexagon_id': pop_indices})\n",
    "    hexagon_total_lengths = temp_df.groupby('hexagon_id')['edge_length'].sum()\n",
    "    temp_df['hexagon_total_length'] = temp_df['hexagon_id'].map(hexagon_total_lengths)\n",
    "    normalized_length_weight = temp_df['edge_length'] / temp_df['hexagon_total_length']\n",
    "    # square root because weight will be applied once at source and once at destination\n",
    "    pop_weight = ((temp_df['population'] / (temp_df['population'].max() + 1)) + 0.01)**0.5\n",
    "\n",
    "    final_weights = lengths * prox_weight * pop_weight\n",
    "\n",
    "    # do random selections\n",
    "    probabilities = final_weights / np.sum(final_weights)\n",
    "    edge_indices = np.arange(len(all_edges))\n",
    "    all_edges_array = np.array(all_edges)\n",
    "    chosen_indices = np.random.choice(edge_indices, size=(num_iterations, 2), p=probabilities)\n",
    "    source_edges = all_edges_array[chosen_indices[:, 0]]\n",
    "    target_edges = all_edges_array[chosen_indices[:, 1]]\n",
    "    source_endpoint_choices = np.random.randint(2, size=num_iterations)\n",
    "    target_endpoint_choices = np.random.randint(2, size=num_iterations)\n",
    "    source_nodes = source_edges[np.arange(num_iterations), source_endpoint_choices]\n",
    "    target_nodes = target_edges[np.arange(num_iterations), target_endpoint_choices]\n",
    "    # calculate distances\n",
    "    dists = np.linalg.norm(source_nodes - target_nodes, axis=1)\n",
    "    # filter out bad paths\n",
    "    acceptance_mask = dists <= shortDistance\n",
    "    long_dists = dists[~acceptance_mask]\n",
    "    probs_long = (shortDistance / long_dists) ** exponent\n",
    "    rolls_long = np.random.random(size=len(long_dists))\n",
    "    acceptance_mask[~acceptance_mask] = rolls_long < probs_long\n",
    "    final_source_nodes = source_nodes[acceptance_mask]\n",
    "    final_target_nodes = target_nodes[acceptance_mask]\n",
    "    successful_selections = len(final_source_nodes)\n",
    "    print(f\"Sampling complete. {successful_selections} pairs were accepted for pathfinding.\")\n",
    "\n",
    "    print(\"Running pathfinding on accepted pairs...\")\n",
    "    start_time = time.time()\n",
    "    successful_paths = 0\n",
    "    failed_paths = 0\n",
    "\n",
    "    parallel = successful_selections > 5000    \n",
    "    if parallel:\n",
    "        print('running parallel...')\n",
    "        pathfinding_jobs = list(zip(final_source_nodes, final_target_nodes))\n",
    "        worker_func = partial(pathfind, graph=G, weight_func = jitter_sqrt, \n",
    "            heuristic_func = euclidean_dist, exponent_val=exponent)\n",
    "\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            results = pool.map(worker_func, pathfinding_jobs)\n",
    "        successful_paths = 0\n",
    "        for result in results:\n",
    "            if result is not None:\n",
    "                path_edges, influence = result\n",
    "                successful_paths += 1\n",
    "                for u, v in path_edges:\n",
    "                    G.edges[u, v]['usage'] += influence\n",
    "    else: \n",
    "        for i, (source_node, target_node) in enumerate(zip(final_source_nodes, final_target_nodes)):\n",
    "            try:\n",
    "                path = nx.astar_path(G, tuple(source_node), tuple(target_node), \n",
    "                                    weight=jitter_sqrt, heuristic=euclidean_dist)\n",
    "\n",
    "                network_dist = sum(G.edges[u, v]['length'] for u, v in zip(path[:-1], path[1:]))\n",
    "                distance = np.linalg.norm(np.array(source_node) - np.array(target_node))\n",
    "                directness = distance / (network_dist + 0.001)\n",
    "                if directness >= SUBWAY_SPEED:\n",
    "                    print ('directness ' + str(directness))\n",
    "                for u, v in zip(path[:-1], path[1:]):\n",
    "                    G.edges[u, v]['usage'] += directness**exponent\n",
    "                successful_paths += 1\n",
    "\n",
    "                if successful_paths > 0 and successful_paths % 1000 == 0:\n",
    "                    print(f\"  ...found {successful_paths}/{successful_selections} paths...\")\n",
    "\n",
    "            except nx.NetworkXNoPath:\n",
    "                failed_paths += 1\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Pathfinding complete. {successful_paths} successful paths found. took {end_time-start_time:.2f} seconds, {(end_time-start_time)/successful_paths:5f} each.\")\n",
    "    \n",
    "\n",
    "    # --- Step 4: Create Final GeoDataFrame ---\n",
    "    # --- 4a: Start with the original street geometries from G_proj ---\n",
    "    df_streets = ox.graph_to_gdfs(G_proj, nodes=False)\n",
    "\n",
    "    # Map the calculated 'usage' from your main graph G back to this GeoDataFrame\n",
    "    print(\"Mapping usage counts to street geometries...\")\n",
    "    usage_counts = []\n",
    "    for u_osmid, v_osmid, data in G_proj.edges(data=True, keys=False):\n",
    "        # Get the coordinate-based nodes used in your main graph G\n",
    "        u_coords = (G_initial.nodes[u_osmid]['x'], G_initial.nodes[u_osmid]['y'])\n",
    "        v_coords = (G_initial.nodes[v_osmid]['x'], G_initial.nodes[v_osmid]['y'])\n",
    "        # Look up the edge in G to find its usage\n",
    "        edge_data = G.get_edge_data(u_coords, v_coords)\n",
    "        if edge_data:\n",
    "            usage_counts.append(edge_data.get('usage', 0))\n",
    "        else: # This might happen if the edge was removed from G, though unlikely here\n",
    "            usage_counts.append(0)\n",
    "    df_streets['usage'] = usage_counts\n",
    "    df_streets['type'] = 'street'\n",
    "    df_streets['route_id'] = None\n",
    "\n",
    "    # --- 4b: Create geometries for all NON-street edges (subway, transfers, etc.) ---\n",
    "    print(\"Creating geometries for subway and other network edges...\")\n",
    "    other_edges_data = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if data.get('type') != 'street':\n",
    "            geom = LineString([u, v])\n",
    "            other_edges_data.append({\n",
    "                'geometry': geom,\n",
    "                'usage': data.get('usage', 0),\n",
    "                'type': data.get('type', 'unknown'),\n",
    "                'route_id': data.get('route_id', None)\n",
    "            })\n",
    "    # Create a GeoDataFrame from the other edges, if any exist\n",
    "    if other_edges_data:\n",
    "        df_other = gpd.GeoDataFrame(other_edges_data, crs=TARGET_CRS)\n",
    "        # --- 4c: Combine the two GeoDataFrames ---\n",
    "        print(\"Combining street and subway GeoDataFrames...\")\n",
    "        df = pd.concat([df_streets, df_other], ignore_index=True)\n",
    "    else:\n",
    "        df = df_streets\n",
    "\n",
    "    # Select only the columns you need for plotting to keep things clean\n",
    "    df = df[['geometry', 'usage', 'type', 'route_id']]\n",
    "\n",
    "    print(\"Final combined GeoDataFrame created successfully.\")\n",
    "    \n",
    "    # save!\n",
    "    df.to_file('tokyo_gdf.gpkg', driver='GPKG')\n",
    "    print('File saved.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    findUsage(4000000)\n",
    "\n",
    "    print('loading file...')\n",
    "    df = gpd.read_file('tokyo_gdf.gpkg')\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Plotting final map...\")\n",
    "    imageSize = 80\n",
    "    fig, ax = plt.subplots(figsize=(imageSize, imageSize))\n",
    "    ax.set_facecolor('black')\n",
    "    ax.set_axis_off()\n",
    "    maxUsage = df.usage.max()\n",
    "\n",
    "    cmap = 'magma'\n",
    "\n",
    "    width_exp = 0.7\n",
    "    streets_df = df[(df['type'] != 'subway') & (df['usage'] > 0)].sort_values('usage', ascending=True)\n",
    "    streets_df.plot(\n",
    "        ax=ax,\n",
    "        column='usage',\n",
    "        cmap=cmap, \n",
    "        linewidth = 0.25 * imageSize * streets_df['usage']**width_exp / maxUsage**width_exp, \n",
    "        norm=LogNorm(vmin=0.3, vmax=df['usage'].max()*0.8),\n",
    "        capstyle='round'    \n",
    "    )\n",
    "    \n",
    "    subway_df = df[df['type'] == 'subway'].copy()\n",
    "    # subway_df['routeColor'] = subway_df['route_id'].map(mta_colors).fillna('#FFFFFF')\n",
    "    # subway_df = subway_df[subway_df['usage'] > 0].sort_values('usage', ascending=True)\n",
    "    # for i in range(10):\n",
    "    #     subway_df.plot(\n",
    "    #         ax=ax,\n",
    "    #         color = subway_df['routeColor'],\n",
    "    #         linewidth = 0.25 * imageSize * subway_df['usage']**width_exp / maxUsage**width_exp, \n",
    "    #         alpha=0.2,\n",
    "    #         #capstyle='round', \n",
    "    #     )\n",
    "    subway_df.plot(\n",
    "        ax=ax,\n",
    "        column='usage',\n",
    "        cmap=cmap, \n",
    "        linewidth = 0.3 * imageSize * subway_df['usage']**width_exp / maxUsage**width_exp, \n",
    "        norm=LogNorm(vmin=0.3, vmax=df['usage'].max()*0.8),\n",
    "        capstyle='round'    \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('nystreets/tokyo' + '.png', bbox_inches='tight', pad_inches=0,\n",
    "    facecolor='black')\n",
    "    print('plot saved!')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
