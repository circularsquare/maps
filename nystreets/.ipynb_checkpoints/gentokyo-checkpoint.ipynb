{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2dc221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm, NoNorm, PowerNorm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.spatial import KDTree\n",
    "from itertools import combinations\n",
    "from pyproj import Transformer\n",
    "from shapely import LineString, MultiLineString\n",
    "from shapely.geometry import Point, box\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0c73f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da0f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBWAY_SPEED = 3 # used in euclidean dist\n",
    "TARGET_CRS = \"EPSG:3095\"\n",
    "\n",
    "def jittered_weight(u, v, data):\n",
    "    jitterSize = 0.03\n",
    "    base_length = data['length']\n",
    "    jitter = random.gauss(1, jitterSize)\n",
    "    return base_length * jitter\n",
    "def jitter_sqrt(u, v, data):\n",
    "    strength = 0.5 # strength of 1 means 10 m path will be jittered by 1 * sqrt(10) = 3\n",
    "                # or 100 m path will get jittered by 1 * sqrt (100) = 10\n",
    "    base_length = data['length']\n",
    "    if base_length <= 0:\n",
    "        return 0.001\n",
    "    sigma = strength * math.sqrt(base_length)\n",
    "    noise = random.gauss(0, sigma)\n",
    "    return max(0.001, base_length + noise)\n",
    "def euclidean_dist(u, v):\n",
    "    return np.linalg.norm(np.array(u) - np.array(v)) / SUBWAY_SPEED \n",
    "\n",
    "def pathfind(source_target_pair, graph, weight_func, heuristic_func, exponent_val):\n",
    "    source_node, target_node = source_target_pair\n",
    "    source_node = tuple(source_node)\n",
    "    target_node = tuple(target_node)\n",
    "    try: \n",
    "        path = nx.astar_path(graph, source_node, target_node, \n",
    "                            weight=weight_func, heuristic=heuristic_func)\n",
    "        network_dist = sum(graph.edges[u, v]['length'] for u, v in zip(path[:-1], path[1:]))\n",
    "        distance = np.linalg.norm(np.array(source_node) - np.array(target_node))\n",
    "        directness = distance / (network_dist + 0.001)\n",
    "        influence = directness ** exponent_val\n",
    "        path_edges = list(zip(path[:-1], path[1:]))\n",
    "        # Return all the edges from this path and the influence score for this path\n",
    "        return (path_edges, influence)\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "def offset_geometries(gdf, offset_distance):\n",
    "    new_geometries = []\n",
    "    # Center the offsets around zero\n",
    "    start_offset = -offset_distance * (len(gdf) - 1) / 2\n",
    "    for i, row in enumerate(gdf.itertuples()):\n",
    "        # Calculate the offset for this specific line\n",
    "        current_offset = start_offset + i * offset_distance\n",
    "        new_geom = row.geometry.parallel_offset(current_offset, 'left', join_style=2)\n",
    "        new_geometries.append(new_geom)\n",
    "    return new_geometries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 2000000\n",
    "\n",
    "\n",
    "print(\"Downloading and building graph from OpenStreetMap...\")\n",
    "place_name = \"Tokyo, Japan\"\n",
    "bbox = (139.546967,35.524403,139.943848,35.842308)\n",
    "center_lon, center_lat = (bbox[0]+bbox[2])/2, (bbox[1]+bbox[3])/2\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", TARGET_CRS, always_xy = True)\n",
    "center_x, center_y = transformer.transform(center_lon, center_lat)\n",
    "center_point = np.array([center_x, center_y])\n",
    "\n",
    "# --- 1. Get data by uncommenting below 4 lines! ---\n",
    "\n",
    "# G_directed = ox.graph_from_bbox(bbox = bbox, network_type='drive')\n",
    "# G_proj = ox.project_graph(G_directed, to_crs=TARGET_CRS)\n",
    "# with open('graph_tokyo.pkl', 'wb') as f:\n",
    "#     pickle.dump(G_proj, f)\n",
    "\n",
    "with open('graph_tokyo.pkl', 'rb') as f:\n",
    "    G_proj = pickle.load(f)\n",
    "G_initial = nx.Graph(G_proj)\n",
    "print(f\"Downloaded graph with {nx.number_connected_components(G_initial)} components.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19ab743a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graph nodes from OSM IDs to coordinates...\n",
      "loading subway data...\n",
      "Downloaded 794 subway stations.\n",
      "Downloaded 1703 subway line segments.\n",
      "Connecting networks at station entrances...\n",
      "Multi-modal graph creation complete!\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Convert Graph Nodes to Coordinate Tuples ---\n",
    "print(\"Converting graph nodes from OSM IDs to coordinates...\")\n",
    "pos = {node: (data['x'], data['y']) for node, data in G_initial.nodes(data=True)}\n",
    "G = nx.Graph() # This will be our final graph with coordinate nodes\n",
    "for u, v, data in G_initial.edges(data=True):\n",
    "    u_coords = pos[u]\n",
    "    v_coords = pos[v]\n",
    "    G.add_edge(u_coords, v_coords, **data)\n",
    "street_nodes = list(G.nodes())\n",
    "\n",
    "# # --- 2.5. subway! ---\n",
    "print('loading subway data...')\n",
    "\n",
    "station_tags = {'railway': 'station', 'station': 'subway'}\n",
    "stations_gdf = ox.features_from_bbox(bbox, station_tags)\n",
    "stations_gdf = stations_gdf.to_crs(TARGET_CRS)\n",
    "# Filter for valid Point geometries\n",
    "stations_gdf = stations_gdf[stations_gdf.geometry.type == 'Point']\n",
    "print(f\"Downloaded {len(stations_gdf)} subway stations.\")\n",
    "\n",
    "line_tags = {'railway': 'subway'}\n",
    "lines_gdf = ox.features_from_bbox(bbox, line_tags)\n",
    "lines_gdf = lines_gdf.to_crs(TARGET_CRS)\n",
    "lines_gdf['route'] = lines_gdf['name:en']\n",
    "print(f\"Downloaded {len(lines_gdf)} subway line segments.\")\n",
    "\n",
    "\n",
    "# prepare street graph G\n",
    "for u, v, data in G.edges(data=True):\n",
    "    G.edges[u, v]['type'] = 'street'\n",
    "\n",
    "subG = nx.Graph()\n",
    "\n",
    "all_subway_data = [] # We'll create a list of dictionaries\n",
    "for index, row in lines_gdf.iterrows():\n",
    "    line = row.geometry\n",
    "    route = row['route'] \n",
    "    segments = [LineString(pair) for pair in zip(line.coords, line.coords[1:])]\n",
    "    for seg in segments:\n",
    "        all_subway_data.append({'geometry': seg, 'route': route})\n",
    "segmented_gdf = gpd.GeoDataFrame(all_subway_data, crs=TARGET_CRS)\n",
    "\n",
    "# --- Part 2: Build the graph from our new, detailed list ---\n",
    "for seg_data in all_subway_data:\n",
    "    segment = seg_data['geometry']\n",
    "    route = seg_data['route'] \n",
    "    start_node = segment.coords[0][:2]\n",
    "    end_node = segment.coords[-1][:2]\n",
    "    if start_node != end_node:\n",
    "        travel_time = segment.length / (SUBWAY_SPEED) #* subway_speeds[route])\n",
    "        subG.add_edge(start_node, end_node,\n",
    "                    length=travel_time, \n",
    "                    type='subway',\n",
    "                    route=route) \n",
    "\n",
    "# 3. find and bridge gaps between segment!\n",
    "endpoints = []\n",
    "for index, row in lines_gdf.iterrows():\n",
    "    line = row.geometry\n",
    "    if len(line.coords) > 1: # Ensure the line is valid\n",
    "        endpoints.append(line.coords[0])\n",
    "        endpoints.append(line.coords[-1])\n",
    "unique_endpoints = list(set(endpoints))\n",
    "endpoint_tree = KDTree(unique_endpoints)\n",
    "\n",
    "# Find all pairs of endpoints that are within 20 meters of each other\n",
    "gap_pairs = endpoint_tree.query_pairs(r=20)\n",
    "for (i, j) in gap_pairs:\n",
    "    node1 = unique_endpoints[i]\n",
    "    node2 = unique_endpoints[j]\n",
    "    dist = euclidean_dist(node1, node2)\n",
    "    subG.add_edge(node1, node2, length = dist / SUBWAY_SPEED, type='subway')\n",
    "\n",
    "G = nx.compose(G, subG)\n",
    "\n",
    "# connecting stations!\n",
    "ENTER_STATION = 30 # meters from station to street (walking is ~ 1 m/s)\n",
    "TRAIN_WAIT = 400 # meters from station to getting on train (applied both ways)\n",
    "\n",
    "print(\"Connecting networks at station entrances...\")\n",
    "station_id_to_node = {}\n",
    "for station in stations_gdf.itertuples():\n",
    "    platform_coords = (station.geometry.x, station.geometry.y)\n",
    "    G.add_node(platform_coords)\n",
    "    station_id_to_node[station.index] = platform_coords\n",
    "\n",
    "subway_nodes = [n for n, d in G.nodes(data=True) if G.degree(n) > 0 and all(G.edges[n, neighbor].get('type') == 'subway' for neighbor in G.neighbors(n))]\n",
    "subway_tree = KDTree(subway_nodes)\n",
    "\n",
    "street_tree = KDTree(street_nodes) # connect station to street\n",
    "for station_id, platform_node in station_id_to_node.items():\n",
    "    dist, idx = street_tree.query(platform_node)\n",
    "    nearest_street_node = street_nodes[idx]\n",
    "    entrance_time = dist + ENTER_STATION\n",
    "    G.add_edge(platform_node, nearest_street_node, length = dist + ENTER_STATION, type='station_entrance')\n",
    "\n",
    "# for station in stations_gdf.itertuples():\n",
    "#     for route in station.routes:\n",
    "#         try:\n",
    "#             # 1. Find ALL possible line segments for the current route\n",
    "#             platform_coords = (station.geometry.x, station.geometry.y)\n",
    "#             platform_point = Point(platform_coords)\n",
    "\n",
    "#             possible_lines = lines_gdf[lines_gdf['route'] == route]\n",
    "#             if possible_lines.empty:\n",
    "#                 print('route not found ' + route)\n",
    "#             route_geom = possible_lines.geometry.union_all()\n",
    "#             track_point = route_geom.interpolate(route_geom.project(platform_point))\n",
    "#             fallback = False\n",
    "#             if track_point:\n",
    "#                 track_node_coords = (track_point.x, track_point.y)\n",
    "#                 dist, idx = subway_tree.query(track_node_coords)\n",
    "#                 nearest_track_node_on_route = subway_nodes[idx]\n",
    "#                 platform_track_dist = euclidean_dist(platform_coords, nearest_track_node_on_route)\n",
    "#             else: \n",
    "#                 fallback = True \n",
    "#             if fallback or platform_track_dist > 200 and route != '5':\n",
    "#                 print(f\"warning: Large platform-to-track distance of {platform_track_dist:.0f}m \"\n",
    "#                     f\"for route {route} at {station.stop_name}\")\n",
    "#                 # fallback if interpolate doesn't work: find closest node\n",
    "#                 route_nodes = []\n",
    "#                 if isinstance(route_geom, MultiLineString):\n",
    "#                     for line in route_geom.geoms:\n",
    "#                         route_nodes.extend(line.coords)\n",
    "#                 else:\n",
    "#                     route_nodes.extend(route_geom.coords)\n",
    "#                 route_tree = KDTree(route_nodes)\n",
    "#                 dist, idx = route_tree.query(platform_coords)\n",
    "#                 nearest_track_node_on_route = route_nodes[idx]\n",
    "#             # Add an edge from the platform to the specific track\n",
    "#             G.add_edge(platform_coords, nearest_track_node_on_route, \n",
    "#                     length=TRAIN_WAIT + platform_track_dist*1.5, type='platform_access')\n",
    "#             #print('added edge ' + route + station.stop_name)\n",
    "#         except (IndexError, KeyError): # if route doesn't have a geometry\n",
    "#             print(\"route doesn't have geometry \" + route)\n",
    "#             pass\n",
    "\n",
    "# adding stop time penalties\n",
    "STOP_TIME = 20 # applied twice while passing a station\n",
    "track_nodes_with_stops = set()\n",
    "for station_id, platform_node in station_id_to_node.items():\n",
    "    for neighbor in G.neighbors(platform_node):\n",
    "        if G.edges[platform_node, neighbor].get('type') == 'platform_access':\n",
    "            track_nodes_with_stops.add(neighbor)\n",
    "for track_node in track_nodes_with_stops:\n",
    "    for neighbor in G.neighbors(track_node):\n",
    "        edge_data = G.edges[track_node, neighbor]\n",
    "        if edge_data.get('type') == 'subway':\n",
    "            edge_data['length'] += STOP_TIME\n",
    "    \n",
    "print(\"Multi-modal graph creation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49480edf-8e2c-4e1b-97cb-2d6396344bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading population...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "data must be of shape (n, m), where there are n points of dimension m",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m pop_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([p\u001b[38;5;241m.\u001b[39mcoords[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pop_centers_gdf\u001b[38;5;241m.\u001b[39mgeometry])\n\u001b[0;32m     16\u001b[0m pop_values \u001b[38;5;241m=\u001b[39m pop_centers_gdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpopulation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 17\u001b[0m pop_tree \u001b[38;5;241m=\u001b[39m KDTree(pop_points)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData loaded successfully. Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pop_gdf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m population grid cells.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\spatial\\_kdtree.py:360\u001b[0m, in \u001b[0;36mKDTree.__init__\u001b[1;34m(self, data, leafsize, compact_nodes, copy_data, balanced_tree, boxsize)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKDTree does not work with complex data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# Note KDTree has different default leafsize from cKDTree\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(data, leafsize, compact_nodes, copy_data,\n\u001b[0;32m    361\u001b[0m                  balanced_tree, boxsize)\n",
      "File \u001b[1;32m_ckdtree.pyx:560\u001b[0m, in \u001b[0;36mscipy.spatial._ckdtree.cKDTree.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: data must be of shape (n, m), where there are n points of dimension m"
     ]
    }
   ],
   "source": [
    "# --- 2.6 load population! --\n",
    "print('loading population...')\n",
    "kontur_filepath = \"../data/nystreets/kontur_population_JP_20231101.gpkg\"\n",
    "west, south, east, north = bbox \n",
    "bbox_gdf_latlon = gpd.GeoDataFrame(\n",
    "    geometry=[box(west, south, east, north)],\n",
    "    crs=\"EPSG:4326\")\n",
    "bbox_gdf_projected = bbox_gdf_latlon.to_crs(TARGET_CRS)\n",
    "projected_bbox_coords = tuple(bbox_gdf_projected.total_bounds)\n",
    "pop_gdf = gpd.read_file(\n",
    "    kontur_filepath).to_crs(TARGET_CRS)\n",
    "pop_centers_gdf = pop_gdf.copy() # collapse to centers\n",
    "pop_centers_gdf['geometry'] = pop_centers_gdf.geometry.centroid\n",
    "pop_points = np.array([p.coords[0] for p in pop_centers_gdf.geometry])\n",
    "pop_values = pop_centers_gdf['population'].values\n",
    "pop_tree = KDTree(pop_points)\n",
    "print(f\"Data loaded successfully. Found {len(pop_gdf)} population grid cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1dab58dc-8893-4475-a5d5-e14e467ce326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h3</th>\n",
       "      <th>population</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty GeoDataFrame\n",
       "Columns: [h3, population, geometry]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d3868-704f-4072-92f6-becc13406b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eda268b-28a6-447b-9be9-683656c04d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af857b82-3c7e-47f2-b0a4-fbd2feaf061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Run Pathfinding Simulation ---\n",
    "print(\"Selecting paths...\")\n",
    "all_nodes = list(G.nodes)\n",
    "all_edges = list(G.edges)\n",
    "final_weights = []\n",
    "for u, v, data in G.edges(data=True):\n",
    "    G.edges[u, v]['usage'] = 0\n",
    "    # G.edges[u, v]['length'] = G.edges[u, v]['length']\n",
    "\n",
    "\n",
    "shortDistance = 1000\n",
    "exponent = 2.3  \n",
    "proximity_sigma = 27000 # 1.6 km per mile\n",
    "\n",
    "# calculate probability of being picked of edges\n",
    "u_coords = np.array([u for u, v in all_edges])\n",
    "v_coords = np.array([v for u, v in all_edges])\n",
    "midpoints = (u_coords + v_coords)/2\n",
    "lengths = np.array(list(nx.get_edge_attributes(G, 'length').values()))\n",
    "# dist_center = np.linalg.norm(u_coords - center_point, axis=1)\n",
    "prox_weight = 1 #np.exp(-(dist_center**2) / (2 * proximity_sigma**2))\n",
    "\n",
    "pop_distances, pop_indices = pop_tree.query(midpoints, k=1)\n",
    "edge_pops = pop_values[pop_indices]\n",
    "# normalize by total edge length in hexagon\n",
    "temp_df = pd.DataFrame({'edge_length': lengths, 'population': edge_pops, 'hexagon_id': pop_indices})\n",
    "hexagon_total_lengths = temp_df.groupby('hexagon_id')['edge_length'].sum()\n",
    "temp_df['hexagon_total_length'] = temp_df['hexagon_id'].map(hexagon_total_lengths)\n",
    "normalized_length_weight = temp_df['edge_length'] / temp_df['hexagon_total_length']\n",
    "# square root because weight will be applied once at source and once at destination\n",
    "pop_weight = ((temp_df['population'] / (temp_df['population'].max() + 1)) + 0.01)**0.5\n",
    "\n",
    "final_weights = lengths * prox_weight * pop_weight\n",
    "\n",
    "# do random selections\n",
    "probabilities = final_weights / np.sum(final_weights)\n",
    "edge_indices = np.arange(len(all_edges))\n",
    "all_edges_array = np.array(all_edges)\n",
    "chosen_indices = np.random.choice(edge_indices, size=(num_iterations, 2), p=probabilities)\n",
    "source_edges = all_edges_array[chosen_indices[:, 0]]\n",
    "target_edges = all_edges_array[chosen_indices[:, 1]]\n",
    "source_endpoint_choices = np.random.randint(2, size=num_iterations)\n",
    "target_endpoint_choices = np.random.randint(2, size=num_iterations)\n",
    "source_nodes = source_edges[np.arange(num_iterations), source_endpoint_choices]\n",
    "target_nodes = target_edges[np.arange(num_iterations), target_endpoint_choices]\n",
    "# calculate distances\n",
    "dists = np.linalg.norm(source_nodes - target_nodes, axis=1)\n",
    "# filter out bad paths\n",
    "acceptance_mask = dists <= shortDistance\n",
    "long_dists = dists[~acceptance_mask]\n",
    "probs_long = (shortDistance / long_dists) ** exponent\n",
    "rolls_long = np.random.random(size=len(long_dists))\n",
    "acceptance_mask[~acceptance_mask] = rolls_long < probs_long\n",
    "final_source_nodes = source_nodes[acceptance_mask]\n",
    "final_target_nodes = target_nodes[acceptance_mask]\n",
    "successful_selections = len(final_source_nodes)\n",
    "print(f\"Sampling complete. {successful_selections} pairs were accepted for pathfinding.\")\n",
    "\n",
    "print(\"Running pathfinding on accepted pairs...\")\n",
    "start_time = time.time()\n",
    "successful_paths = 0\n",
    "failed_paths = 0\n",
    "\n",
    "parallel = successful_selections > 5000    \n",
    "if parallel:\n",
    "    print('running parallel...')\n",
    "    pathfinding_jobs = list(zip(final_source_nodes, final_target_nodes))\n",
    "    worker_func = partial(pathfind, graph=G, weight_func = jitter_sqrt, \n",
    "        heuristic_func = euclidean_dist, exponent_val=exponent)\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        results = pool.map(worker_func, pathfinding_jobs)\n",
    "    successful_paths = 0\n",
    "    for result in results:\n",
    "        if result is not None:\n",
    "            path_edges, influence = result\n",
    "            successful_paths += 1\n",
    "            for u, v in path_edges:\n",
    "                G.edges[u, v]['usage'] += influence\n",
    "else: \n",
    "    for i, (source_node, target_node) in enumerate(zip(final_source_nodes, final_target_nodes)):\n",
    "        try:\n",
    "            path = nx.astar_path(G, tuple(source_node), tuple(target_node), \n",
    "                                weight=jitter_sqrt, heuristic=euclidean_dist)\n",
    "\n",
    "            network_dist = sum(G.edges[u, v]['length'] for u, v in zip(path[:-1], path[1:]))\n",
    "            distance = np.linalg.norm(np.array(source_node) - np.array(target_node))\n",
    "            directness = distance / (network_dist + 0.001)\n",
    "            if directness >= SUBWAY_SPEED:\n",
    "                print ('directness ' + str(directness))\n",
    "            for u, v in zip(path[:-1], path[1:]):\n",
    "                G.edges[u, v]['usage'] += directness**exponent\n",
    "            successful_paths += 1\n",
    "\n",
    "            if successful_paths > 0 and successful_paths % 1000 == 0:\n",
    "                print(f\"  ...found {successful_paths}/{successful_selections} paths...\")\n",
    "\n",
    "        except nx.NetworkXNoPath:\n",
    "            failed_paths += 1\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Pathfinding complete. {successful_paths} successful paths found. took {end_time-start_time:.2f} seconds, {(end_time-start_time)/successful_paths:5f} each.\")\n",
    "\n",
    "\n",
    "# --- Step 4: Create Final GeoDataFrame ---\n",
    "# --- 4a: Start with the original street geometries from G_proj ---\n",
    "df_streets = ox.graph_to_gdfs(G_proj, nodes=False)\n",
    "\n",
    "# Map the calculated 'usage' from your main graph G back to this GeoDataFrame\n",
    "print(\"Mapping usage counts to street geometries...\")\n",
    "usage_counts = []\n",
    "for u_osmid, v_osmid, data in G_proj.edges(data=True, keys=False):\n",
    "    # Get the coordinate-based nodes used in youzr main graph G\n",
    "    u_coords = (G_initial.nodes[u_osmid]['x'], G_initial.nodes[u_osmid]['y'])\n",
    "    v_coords = (G_initial.nodes[v_osmid]['x'], G_initial.nodes[v_osmid]['y'])\n",
    "    # Look up the edge in G to find its usage\n",
    "    edge_data = G.get_edge_data(u_coords, v_coords)\n",
    "    if edge_data:\n",
    "        usage_counts.append(edge_data.get('usage', 0))\n",
    "    else: # This might happen if the edge was removed from G, though unlikely here\n",
    "        usage_counts.append(0)\n",
    "df_streets['usage'] = usage_counts\n",
    "df_streets['type'] = 'street'\n",
    "df_streets['route'] = None\n",
    "\n",
    "# --- 4b: Create geometries for all NON-street edges (subway, transfers, etc.) ---\n",
    "print(\"Creating geometries for subway and other network edges...\")\n",
    "other_edges_data = []\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if data.get('type') != 'street':\n",
    "        geom = LineString([u, v])\n",
    "        other_edges_data.append({\n",
    "            'geometry': geom,\n",
    "            'usage': data.get('usage', 0),\n",
    "            'type': data.get('type', 'unknown'),\n",
    "            'route': data.get('route', None)\n",
    "        })\n",
    "# Create a GeoDataFrame from the other edges, if any exist\n",
    "if other_edges_data:\n",
    "    df_other = gpd.GeoDataFrame(other_edges_data, crs=TARGET_CRS)\n",
    "    # --- 4c: Combine the two GeoDataFrames ---\n",
    "    print(\"Combining street and subway GeoDataFrames...\")\n",
    "    df = pd.concat([df_streets, df_other], ignore_index=True)\n",
    "else:\n",
    "    df = df_streets\n",
    "\n",
    "# Select only the columns you need for plotting to keep things clean\n",
    "df = df[['geometry', 'usage', 'type', 'route']]\n",
    "\n",
    "print(\"Final combined GeoDataFrame created successfully.\")\n",
    "\n",
    "# save!\n",
    "df.to_file('tokyo_gdf.gpkg', driver='GPKG')\n",
    "print('File saved.')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "print('loading file...')\n",
    "df = gpd.read_file('tokyo_gdf.gpkg')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Plotting final map...\")\n",
    "imageSize = 80\n",
    "fig, ax = plt.subplots(figsize=(imageSize, imageSize))\n",
    "ax.set_facecolor('black')\n",
    "ax.set_axis_off()\n",
    "maxUsage = df.usage.max()\n",
    "\n",
    "cmap = 'magma'\n",
    "\n",
    "width_exp = 0.7\n",
    "streets_df = df[(df['type'] != 'subway') & (df['usage'] > 0)].sort_values('usage', ascending=True)\n",
    "streets_df.plot(\n",
    "    ax=ax,\n",
    "    column='usage',\n",
    "    cmap=cmap, \n",
    "    linewidth = 0.25 * imageSize * streets_df['usage']**width_exp / maxUsage**width_exp, \n",
    "    norm=LogNorm(vmin=0.3, vmax=df['usage'].max()*0.8),\n",
    "    capstyle='round'    \n",
    ")\n",
    "\n",
    "subway_df = df[df['type'] == 'subway'].copy()\n",
    "# subway_df['routeColor'] = subway_df['route'].map(mta_colors).fillna('#FFFFFF')\n",
    "# subway_df = subway_df[subway_df['usage'] > 0].sort_values('usage', ascending=True)\n",
    "# for i in range(10):\n",
    "#     subway_df.plot(\n",
    "#         ax=ax,\n",
    "#         color = subway_df['routeColor'],\n",
    "#         linewidth = 0.25 * imageSize * subway_df['usage']**width_exp / maxUsage**width_exp, \n",
    "#         alpha=0.2,\n",
    "#         #capstyle='round', \n",
    "#     )\n",
    "subway_df.plot(\n",
    "    ax=ax,\n",
    "    column='usage',\n",
    "    cmap=cmap, \n",
    "    linewidth = 0.3 * imageSize * subway_df['usage']**width_exp / maxUsage**width_exp, \n",
    "    norm=LogNorm(vmin=0.3, vmax=df['usage'].max()*0.8),\n",
    "    capstyle='round'    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nystreets/tokyo' + '.png', bbox_inches='tight', pad_inches=0,\n",
    "facecolor='black')\n",
    "print('plot saved!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df249a-3997-4b37-b28b-f5a5065f7455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd36ee-6ae1-4df8-97d8-1500c51ae290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823bc8d1-23ea-4e4d-8e96-5307d3688b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be8f5a3-e731-4dc6-afbc-e4c91d034eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef372865-c8cb-431e-8083-5d4420950b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
